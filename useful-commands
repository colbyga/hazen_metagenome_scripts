#######
# using awk
#######
# looking for rows with a second field value greater than 1000
# {print ;} print entire line
awk '$2 > 1000' filename
awk '$4 > 49  {print ;}' collections-refined.txt
awk '$4 > 90  {print ;}' collections-refined.txt | wc -l  # insane quality bins
awk '$4 > 49 &&  $5 > 10 {print ;}' collections-refined.txt | wc -l  # Good bins but contaminated
awk '$5 > 10 {print ;}' collections-refined.txt | wc -l  


awk '{print $13}' qa.out
awk '$13 > 90  {print ;}' qa.out | wc -l  # insane quality bins
awk '$13 > 49 &&  $14 > 10 {print ;}' qa.out | wc -l  # Good bins but contaminated
awk '$13 > 49 &&  $14 > 10 {print ;}' qa.out | awk '{print $1}' # oh look all the contaminated bin names that need work 
awk '$13 > 49 &&  $14 > 10 {print ;}' qa.out | awk '{print $13 "\t" $14}' 
awk '$13 > 49 &&  $14 < 10 {print ;}' qa.out > qa_goodbins.out
awk '$13 > 90 &&  $14 > 10 {print ;}' qa.out | wc -l  # Good bins but contaminated
awk '$13 > 70 &&  $14 > 10 {print ;}' qa.out | wc -l  # Good bins but contaminated



#print specific field
awk '{print $1}' collections-refined-over-49.txt > bins-over-49.txt

######
# using sed 
######
sed -i -e 's/few/asd/g' hello.txt
sed 's/hello/bonjour/' greetings.txt
#using ; as a separator
sed 's;Bin_;$BINS/Bin_;' bins-over-49.txt > bins-over-49.txt

tr '\n' ' ' < bins-over-49.txt > bins-over-49-space.txt

############
# renaming files using a mapping file 
############
awk '$1 &&  $2 {print ;}' mapping-text.csv > execute.map
awk '{print $2}' | sed 's/.json/-contigs.fa/g'

sed 's/^//g' files.csv | while IFS=, read orig new; do mv "$orig" "$new"; done 



sed -i -e 's/^/mv /' map2.txt
awk -F',' 'system("mv " $1 " " $2)' execute.map

awk '{print "mv "$0}' execute.map
sed 's/^/mv /' execute.map > execute.map.txt

while read p; do
  echo mv "$p" >> execute.map.final
done <execute.map;



for line in execute.map; do echo mv $line >> execute.map.final; done;

for line; do
  echo mv "$line"
done < execute.map

sed -e 's/^/prefix/' file

# If you want to edit the file in-place
sed -i -e 's/^/mv /' execute.map

# If you want to create a new file
sed -e 's/^/prefix/' file > file.new


############
#  copying files over via list 
############

for direct in $(ls *S*); do echo $direct; done;

for direct in $(ls *S*); 
  do while read p; 
    do cp /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/jsons_files/$p-rgi.out.json /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/by_samples/$direct;
  done < /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/by_samples/bins-list/"$direct"-bins;
done;

while read p; 
do cp /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/jsons_files/$p-rgi.out.json /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/by_samples/AS;
done < AS-bins

while read p; 
do cp /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/jsons_files/$p-rgi.out.json /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/rgi-all-json/by_samples/BS;
done < BS-bins



sed -e 's/^/cp -s /' AS-bins > AS-copy



############
#  bringing it together
############
awk '$13 > 49 &&  $14 < 10 {print ;}' qa.out | awk '{print $1}' | sed 's/-contigs/-contigs.fa/g'
awk '$13 > 49 &&  $14 < 10 {print ;}' qa.out | awk '{print $1}' | sed 's/-contigs//g' > goodbins.names

while read p; do
  echo "$p"
done <goodbins.names

while read p; do
  cp -s /global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/bin_by_bin/$p/$p-contigs.fa /global/scratch/hpc3565//output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/good_bins
done < goodbins.names

cp -s

######
# using grep 
######
# grep in reverse -v is reverse -w is whole word -i case insensitive -E allows for regrex
grep -vwiE "(superfamily|panther|pirsf)" temp-func-matrix.tsv > filename.tsv
grep -vwE "(cat|rat)" sourcefile > destinationfile


#bash loop
for filename in $(ls *.hmm); do  echo $pwd$filename >> hmm-list.txt; done;


rerunning RGI that didn't run
seq 2 878 > all-errs.txt
cat all-errs.txt | while read f; do echo rgi_1832827_$f.err; done > all-list-of-err.txt
# finding the difference between two files
grep -v -f list-of-err.txt all-list-of-err.txt
#786 files and should be 878
sed 's/rgi_1832827_//' ah-whats-missing.txt > ah-whats-missing2.txt
sed 's/.err//' ah-whats-missing2.txt > ah-whats-missing3.txt
tr '\n' ',' < ah-whats-missing3.txt
ls rgi_1834603* | wc -l #should be 92!


repeating the second time!
sed 's/rgi_1832827_//' list-slurm-errs.txt > list-slurm-errs-2.txt 
sed 's/rgi_1834603_//' list-slurm-errs-2.txt > list-slurm-errs-3.txt  
grep -v -fxF list-slurm-errs-3.txt all-list-of-err.txt
# below is the golden pattern match
grep -vFwf list-slurm-errs.txt all-list-of-err.txt > matches.txt

#grep -Fwfv all-list-of-err.txt list-slurm-errs.txt
#while read i; do grep "$i" fileB; done < fileA


# now for minpath
repeating the second time!
ls *err > all-slurm-errs.txt
sed 's/minpath_1838520_//' all-slurm-errs.txt > all-slurm-errs2.txt
seq 2 878> all-errs.txt
cat all-errs.txt | while read f; do echo minpath_$f.err; done > all-list-of-err.txt

grep -v -fxF list-slurm-errs-3.txt all-list-of-err.txt
# below is the golden pattern match
grep -vFwf all-slurm-errs.txt all-list-of-err.txt > matches.txt
sed 's/.err//g' matches.txt > a-rerun.txt
sed 's/minpath_//g' a-rerun.txt > b-rerun.txt
tr '\n' ',' < b-rerun.txt

###########
# extracting only goodbins from the anvio bins_summary.txt
###########
grep -vFwf goodbins.names bins_summary.txt  ### output 570 lines 
grep -Fwf goodbins.names bins_summary.txt   ### output 308 lines
# goodbin abundance
grep -Fwf goodbins.names abundance.txt > processed_abundance.txt
awk '{if(NR==1) print $0}' abundance.txt | cat - processed_abundance.txt > temp && mv temp processed_abundance.txt

grep -Fwf goodbins.names mean_coverage.txt > processed_mean_coverage.txt
awk '{if(NR==1) print $0}' mean_coverage.txt | cat - processed_mean_coverage.txt > temp && mv temp processed_mean_coverage.txt

###########
# extracting only goodbins from the checkm profile3.tsv in output_anvio5_1/SAMPLES-SUMMARY-concoct-checkm/processed-bins_across_samples 
###########
sed -i '/-contigs/ s///g' profile3.tsv
grep -Fwf goodbins.names profile3.tsv > processed_profile.tsv   ### output 308 lines
grep -Fwf goodbins.names mean_coverage.txt > processed_mean_coverage.txt   ### output 308 lines



##################
#Compressing files
##################
tar -czvf archive.tar.gz stuff
tar -cjvf archive.tar.bz2 stuff

#working with compressed files 
#extracting a specific file from a tar compressed file 
tar -zxvf <tar filename> <file you want to extract> ### if its tar.gz
tar -jxvf output_anvio5_1.tar.bz2 --strip-components=4 global/scratch/hpc3565/output_anvio5_1/contigs.db -C /Users/grahamcolby/Documents/anvio_refining-post-checkm
tar -jxvf output_anvio5_1.tar.bz2 --strip-components=4 global/scratch/hpc3565/output_anvio5_1/SAMPLES-MERGED -C /Users/grahamcolby/Documents/anvio_refining-post-checkm
tar -jxvf output_anvio5_1.tar.bz2 --strip-components=4 global/scratch/hpc3565/output_anvio5_1/SAMPLES-SUMMARY-concoct


list files
tar -jtvf archive.tar.bz2
tar --bzip2 --list --verbose --file=archive.tar

############
# looking at raw data files
############
# number of reads in fastq.gc file 
zcat my.fastq.gz | echo $((`wc -l`/4))
#or 
cat my.sam | grep -v '^ *@' | wc -l


#count lines in fasta file 
grep -c "^>" file.fa

